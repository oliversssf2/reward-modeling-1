# model_path: "Dahoas/gptneo-sft-static"
model_path: "EleutherAI/gpt-neo-125M"

model_type: "causal"
# tokenizer_path: "EleutherAI/gpt-neo-1.3B"
# tokenizer_path: "EleutherAI/gpt-neo-125M"
num_layers_unfrozen: 0.67

tokenizer:
  pretrained_model_name_or_path: "EleutherAI/gpt-neo-125M"

train_args:
  # output_dir: "/fsx/alex/ckpts/gptneo-rm"
  output_dir: "./ckpts/gptneo-rm"
  num_train_epochs: 10
  logging_steps: 100
  save_strategy: "steps"
  save_steps: 2000
  # load_best_model_at_end: True
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  warmup_steps: 100
  weight_decay: 0.01
  learning_rate: 1.0e-5
  save_total_limit: 5
  logging_dir: "./logs"
  fp16: False
  bf16: True
  evaluation_strategy: "steps"
  eval_steps: 2000
  # report_to: "tensorboard"

data_path: "Anthropic/hh-rlhf"
data_dir: "harmless-base"

trainer_type: "sparse"


